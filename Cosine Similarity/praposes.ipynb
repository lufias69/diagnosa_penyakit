{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fuzzywuzzy import fuzz as sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = sim.token_set_ratio('Room, 2 Double Beds (19th to 25th Floors)', 'Two Double Beds - Location Room (19th to 25th Floors)')\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from fuzzywuzzy import fuzz as sim\n",
    "# def prediksi(gejala, namafile):\n",
    "# #     print(namafile)\n",
    "#     data = pd.read_excel(namafile)\n",
    "#     gejala_list = data[\"Tanda_dan_Gejala\"].tolist()\n",
    "#     diagnosa_list = data[\"Nama Penyakit\"].tolist()\n",
    "#     detail = data[\"Detail Nama Penyakit\"].tolist()\n",
    "# #     cek = gejala\n",
    "# #     data_latih = [\"a\", \"b\"]\n",
    "# #     diagnosa = ['aa','bb']\n",
    "#     ratio=list()\n",
    "#     for i in gejala_list:\n",
    "#         rat = fuzz.token_set_ratio(i, gejala)\n",
    "#         ratio.append(rat)\n",
    "\n",
    "#     dict_ = {\n",
    "#         \"similarity\":ratio,\n",
    "#         \"diagnosa\":diagnosa_list,\n",
    "#         \"detail\":detail\n",
    "#     }\n",
    "\n",
    "\n",
    "#     dataf = pd.DataFrame.from_dict(dict_)\n",
    "# #     print(dataf)\n",
    "# #     sorted_ = dataf.sort_values(by=['similarity'], descending=True)\n",
    "#     sorted_ =  dataf.sort_values(by=['similarity'], ascending=False)\n",
    "# #     print(sorted_)\n",
    "#     diagnosa = sorted_['diagnosa'].tolist()[0]\n",
    "#     similarity = sorted_['similarity'].tolist()[0]\n",
    "#     detail = sorted_['detail'].tolist()[0]\n",
    "    \n",
    "#     return [diagnosa, similarity, detail, sorted_]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_train_infection\n",
    "# namafile = \"data_train_infection.xlsx\"\n",
    "\n",
    "\n",
    "# gejala = \"Nausea dan vomitus. Penurunan selera makan. Demam. Berkembang hingga menimbulkan nyeri abdomen, muntah darah, dan diare hebat.\"\n",
    "# print(\"Tanda dan Gejala :\",gejala)\n",
    "# print(\"similarity       :\",prediksi(gejala, namafile)[1])\n",
    "# print(\"Nama Penyakit    :\",prediksi(gejala, namafile)[0])\n",
    "# print(\"Detail  Penyakit :\",prediksi(gejala, namafile)[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from tokenisasi import  tokenisasi as ts\n",
    "import json\n",
    "from joblib import dump\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"data_train_infection.xlsx\")\n",
    "gejala_list = data[\"Tanda_dan_Gejala\"].tolist()\n",
    "diagnosa_list = data[\"Nama Penyakit\"].tolist()\n",
    "detail = data[\"Detail Nama Penyakit\"].tolist()\n",
    "Jenis_Infeksi = data[\"Jenis Infeksi\"].tolist()\n",
    "\n",
    "dict_ = {\n",
    "    \"gejala\":gejala_list,\n",
    "    \"nama_penyakit\":diagnosa_list,\n",
    "    \"detail\":detail,\n",
    "    \"Jenis_Infeksi\":Jenis_Infeksi\n",
    "}\n",
    "with open(\"prediksi\\model\\penyakit.json\",\"w\")as f:\n",
    "    json.dump(dict_, f)\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from joblib import dump\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# corpus = gejala_list\n",
    "# vectorizer = TfidfVectorizer()\n",
    "# X = vectorizer.fit_transform(corpus)\n",
    "# save = vectorizer.fit(corpus)\n",
    "# # dump(save, 'prediksi/model/tfidf_model.sav')\n",
    "# # print(vectorizer.get_feature_names())\n",
    "# X = X.toarray()\n",
    "# # print(X.shape)\n",
    "# X = X.tolist()\n",
    "# # with open(\"prediksi/model/vector_model.json\",\"w\")as f:\n",
    "# #     json.dump(X, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = X.tolist()\n",
    "# with open(\"prediksi/model/vector_model.json\",\"w\")as f:\n",
    "#     json.dump(X, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "n = list()\n",
    "for i in gejala_list:\n",
    "    t_kata = tknzr.tokenize(i)\n",
    "    for j in t_kata:\n",
    "        n.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for i in list(set(n)):\n",
    "#     print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "di|oleh|atau|dan|\"|/|)|awalnya|(|-|ada|ini|lagi|,|itu|)|:|.|dengan|disertai|dari|karena|yang|pada|serta|dapat|untuk|hingga|"
     ]
    }
   ],
   "source": [
    "deli = \"\"\"\n",
    "di\n",
    "oleh\n",
    "atau\n",
    "dan\n",
    "\"\n",
    "/\n",
    ")\n",
    "awalnya\n",
    "(\n",
    "-\n",
    "ada\n",
    "ini\n",
    "lagi\n",
    ",\n",
    "itu\n",
    ")\n",
    ":\n",
    ".\n",
    "dengan\n",
    "disertai\n",
    "dari\n",
    "karena\n",
    "yang\n",
    "pada\n",
    "serta\n",
    "dapat\n",
    "untuk\n",
    "hingga\n",
    "\"\"\"\n",
    "\n",
    "for i in deli.split():\n",
    "    print(i, end=\"|\")\n",
    "deli = deli.split()\n",
    "with open(\"prediksi/model/delimiter.json\", \"w\") as f:\n",
    "    json.dump(deli, f)\n",
    "new_gejala = list()\n",
    "for i in gejala_list:\n",
    "    new_gejala.append((ts.ngramku(i, n=10, delimiter = deli)['string'].lower()))\n",
    "    \n",
    "corpus = new_gejala\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "save = vectorizer.fit(corpus)\n",
    "dump(save, 'prediksi/model/tfidf_model.sav')\n",
    "# print(vectorizer.get_feature_names())\n",
    "X_ = X.toarray()\n",
    "# print(X.shape)\n",
    "X = X_.tolist()\n",
    "with open(\"prediksi/model/vector_model.json\",\"w\")as f:\n",
    "    json.dump(X, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function dump at 0x000001C14B5D42F0>\n"
     ]
    }
   ],
   "source": [
    "# samples = X_\n",
    "from sklearn.neighbors import NearestNeighbors, KNeighborsClassifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "neigh.fit(X_, diagnosa_list)\n",
    "dump(neigh, 'prediksi/model/knn_model_diagnosa.sav')\n",
    "\n",
    "neigh = KNeighborsClassifier(n_neighbors=1)\n",
    "neigh.fit(X_, detail)\n",
    "dump(neigh, 'prediksi/model/knn_model_detail.sav')\n",
    "\n",
    "print(dump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Kutaneus'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x  = neigh.predict([X[0]])#predict([X[0]])\n",
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Antrax'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
